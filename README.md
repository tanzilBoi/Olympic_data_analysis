# ğŸ… Olympic Data Engineering ETL Pipeline using Azure Databricks

## ğŸ“Œ Project Overview

This project demonstrates an end-to-end Data Engineering pipeline built using Azure Databricks and PySpark to process Olympic datasets. The pipeline follows the Medallion Architecture (Bronze, Silver, Gold) to transform raw data into analytics-ready datasets.

The goal of this project is to simulate a real-world ETL pipeline for data ingestion, transformation, and storage.

---

## ğŸ—ï¸ Architecture
This project follows Medallion Architecture (Bronze, Silver, Gold):

!(Olympic_2021/architecture.png)

Bronze Layer â†’ Raw Data Ingestion  
Silver Layer â†’ Cleaned and Transformed Data  
Gold Layer â†’ Analytics Ready Data

---

## âš™ï¸ Technologies Used

- Azure Databricks
- PySpark
- Spark SQL
- Azure Data Lake Storage
- ETL Pipeline
- Medallion Architecture

---

## ğŸ”„ ETL Pipeline Flow

### Bronze Layer
- Ingested raw Olympic dataset from GitHub
- Stored raw data in its original format

### Silver Layer
- Performed data cleaning
- Handled missing and null values
- Standardized column formats

### Gold Layer
- Created curated, analytics-ready datasets
- Applied business transformations
- Prepared data for reporting and analysis

---

## ğŸ“Š Dataset Includes

- Athlete details
- Country information
- Medal counts
- Event details

---

## ğŸš€ Key Features

âœ” End-to-End ETL Pipeline  
âœ” Medallion Architecture Implementation  
âœ” Data Cleaning and Transformation  
âœ” PySpark Data Processing  
âœ” Analytics-ready Data  

---

## ğŸ“‚ Project Structure

```
olympic-data-engineering-project/

notebooks/
etl_pipeline.ipynb

data/
raw/
processed/

images/
architecture.png
```

---

## ğŸ“ˆ Sample Use Case

The final Gold Layer dataset can be used to analyze:

- Medal distribution by country
- Athlete performance trends
- Country-wise Olympic performance

---

## ğŸ‘¨â€ğŸ’» Author

Your Name

LinkedIn: https://linkedin.com/in/tanzil-ameen-23370521a/

GitHub: https://github.com/tanzilBoi

---

## â­ Project Objective

This project was created to gain hands-on experience in building scalable ETL pipelines using Azure Databricks and PySpark and to understand real-world Data Engineering workflows.
